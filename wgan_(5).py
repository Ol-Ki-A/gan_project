# -*- coding: utf-8 -*-
"""wgan (5).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KmClnIu1FPzBX-tAD4q2cFtIs7KjGDMF
"""

#!g1.1
from __future__ import print_function
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch.autograd import Variable
import torch
import torchvision
from torchvision import transforms
import matplotlib.pyplot as plt
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
import torch.optim as optim
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
from tqdm.notebook import tqdm
import os
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
import torchvision.transforms as tt
from torchvision.utils import save_image
from torchvision.utils import make_grid

#!g1.1
batchsize = 64
latent_size = 100

#!g1.1
training_data = dset.CIFAR10(
    root='./data',
    train=True,
    download=True,
    transform=transforms.Compose ([ 
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ])
)

#!g1.1
train_loader = torch.utils.data.DataLoader(training_data, batch_size = batchsize, shuffle=True, num_workers=4, drop_last=True)

#!g1.1
def weight_init(m):
  classname = m.__class__.__name__
  if classname.find('Conv') != -1:
    m.weight.data.normal_(0.0, 0.02)
  elif classname.find('BatchNorm') != -1:
    m.weight.data.normal_(1.0, 0.02)
    m.bias.data.fill_(0)

#!g1.1
#!g1.1 
class Generator(nn.Module): 
  def __init__(self): 
    super(Generator, self).__init__() 
    self.g = nn.Sequential( 
        nn.ConvTranspose2d(100, 1024, 2, 1, 0, bias = False), 
        nn.BatchNorm2d(1024), 
        nn.ReLU(True), 
        nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias = False), 
        nn.BatchNorm2d(512), 
        nn.ReLU(True), 
        nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False), 
        nn.BatchNorm2d(256), 
        nn.ReLU(True), 
        nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False), 
        nn.BatchNorm2d(128), 
        nn.ReLU(True), 
        nn.ConvTranspose2d(128, 3, 4, 2, 1, bias = False), 
        nn.Tanh() 
    ) 
  def forward(self, input): 
    return self.g(input.view(-1, 100, 1, 1)) 
    #fake = self.g(input) 
    #return fake

#!g1.1 
class Discriminator(nn.Module): 
  def __init__(self): 
    super(Discriminator, self).__init__() 
    self.d = nn.Sequential( 
        nn.Conv2d(3, 64, 5, 2, 2, bias = False), 
        nn.LeakyReLU(0.2, inplace = True), 
        nn.Conv2d(64, 128, 5, 2, 2, bias = False), 
        nn.LeakyReLU(0.2, inplace = True),  
        nn.BatchNorm2d(128), 
        nn.Conv2d(128, 256, 5, 2, 2, bias = False), 
        nn.LeakyReLU(0.2, inplace = True), 
        nn.BatchNorm2d(256), 
        nn.Conv2d(256, 512, 5, 2, 2, bias = False), 
        nn.LeakyReLU(0.2, inplace = True), 
        nn.BatchNorm2d(512) 
    ) 
    self.l = nn.Linear(2048, 1) 
  def forward(self, input): 
    ans = self.d(input) 
    #print(ans.shape) 
    ans1 = torch.flatten(ans, start_dim=1) 
    #print(ans1.shape) 
    ans = self.l(ans1) 
    #print(ans.shape) 
    return ans

#!g1.1
netGen = Generator()
netGen.apply(weight_init)

#!g1.1
netDis = Discriminator()
netDis.apply(weight_init)

#!g1.1
optDis = optim.RMSprop(netDis.parameters(), lr=0.00005)
optGen = optim.RMSprop(netGen.parameters(), lr=0.00005)

#!g1.1
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

#!g1.1
netDis.to(device)
netGen.to(device)

#!g1.1
real_score = [] 
fake_score = [] 
epochs = 150
losses_g = [] 
losses_d = [] 
for epoch in range(epochs):
    for real_image, i in tqdm(train_loader):  
        loss_d_tmp = [] 
        loss_g_tmp = [] 
        real_score_tmp = [] 
        fake_score_tmp = []   
        for i in range(5):
            with torch.no_grad():
                latent = torch.randn(real_image.size()[0], latent_size, device=device)
                latent = latent.to(device) 
                #генерируем изображение 
                fake_image = netGen(latent).detach()
            real_image = real_image.to(device) 
            #real_image = Variable(real_image) 
            real_preds = netDis(real_image).reshape(-1)
            fake_preds = netDis(fake_image).reshape(-1)
            loss_critic = -real_preds.mean() + fake_preds.mean()
            #loss_critic = Variable(loss_critic, requires_grad = True)
            optDis.zero_grad() 
            loss_critic.backward(retain_graph=True)
            optDis.step()
            for p in netDis.parameters():
                p.data.clamp_(-0.01, 0.01)
        latent = torch.randn(real_image.size()[0], latent_size, device=device)
        fake_image = netGen(latent)
        fake_image = netDis(fake_image).reshape(-1) 
        gen_loss = -fake_image.mean()
        optGen.zero_grad() 
        gen_loss.backward()
        optGen.step()
        real_score_tmp.append(torch.mean(real_preds).item()) 
        fake_score_tmp.append(torch.mean(fake_preds).item()) 
        loss_g_tmp.append(gen_loss.item()) 
    if (epoch % 10 == 0):   
        with torch.no_grad():
            latent = torch.randn(real_image.size()[0], latent_size, device=device)
            fake_image = netGen(latent).detach()
            vutils.save_image(fake_image.data, '%s/fake_samples_epoch_%03d.png' % ("data/", epoch), normalize = True) 
            if (epoch % 50 == 0 and epoch!=0):   
                dataset = GeneratorDataset(netGen, latent_size)
                loader = DataLoader(dataset, batch_size=64, num_workers=0)
                IS, IS_std = get_inception_score(loader)
                print(IS)
        torch.save(netGen, "model_gen" + str(epoch) + ".zip") 
        torch.save(netDis, "model_dis" + str(epoch) + ".zip") 
        torch.save(optDis, "optDis" + str(epoch) + ".zip") 
        torch.save(optGen, "optGen" + str(epoch) + ".zip") 
        print(f'loss generator on {epoch} = {np.mean(loss_g_tmp)}') 
        print(f'real scores on {epoch} = {np.mean(real_score_tmp)}') 
        print(f'fake scores on {epoch} = {np.mean(fake_score_tmp)}') 
        losses_g.append(np.mean(loss_g_tmp)) 
        losses_d.append(np.mean(loss_d_tmp)) 
        real_score.append(np.mean(real_score_tmp)) 
        fake_score.append(np.mean(fake_score_tmp))