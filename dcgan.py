# -*- coding: utf-8 -*-
"""Untitled (3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1axfIq9W7HDfNHXbSBJkmoGhA9gc0m6fL
"""

#!g1.1
from __future__ import print_function
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch.autograd import Variable
import torch
import torchvision
from torchvision import transforms
import matplotlib.pyplot as plt
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
import torch.optim as optim
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
from tqdm.notebook import tqdm
import os
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
import torchvision.transforms as tt
from torchvision.utils import save_image
from torchvision.utils import make_grid

#!g1.1
batchsize = 64
latent_size = 100

#!g1.1
training_data = dset.CIFAR10(
    root='./data',
    train=True,
    download=True,
    transform=transforms.Compose ([ 
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ])
)

#!g1.1
train_loader = torch.utils.data.DataLoader(training_data, batch_size= batchsize, shuffle=True, num_workers=2)

#!g1.1
def weight_init(m):
  classname = m.__class__.__name__
  if classname.find('Conv') != -1:
    m.weight.data.normal_(0.0, 0.02)
  elif classname.find('BatchNorm') != -1:
    m.weight.data.normal_(1.0, 0.02)
    m.bias.data.fill_(0)

#!g1.1 
class Generator(nn.Module): 
  def __init__(self): 
    super(Generator, self).__init__() 
    self.g = nn.Sequential( 
        nn.ConvTranspose2d(100, 1024, 2, 1, 0, bias = False), 
        nn.BatchNorm2d(1024), 
        nn.ReLU(True), 
        nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias = False), 
        nn.BatchNorm2d(512), 
        nn.ReLU(True), 
        nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False), 
        nn.BatchNorm2d(256), 
        nn.ReLU(True), 
        nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False), 
        nn.BatchNorm2d(128), 
        nn.ReLU(True), 
        nn.ConvTranspose2d(128, 3, 4, 2, 1, bias = False), 
        nn.Tanh() 
    ) 
  def forward(self, input): 
    return self.g(input.view(-1, 100, 1, 1)) 
    #fake = self.g(input) 
    #return fake

#!g1.1 
class Discriminator(nn.Module): 
  def __init__(self): 
    super(Discriminator, self).__init__() 
    self.d = nn.Sequential( 
        nn.Conv2d(3, 64, 5, 2, 2, bias = False), 
        nn.LeakyReLU(0.2, inplace = True), 
        nn.Conv2d(64, 128, 5, 2, 2, bias = False), 
        nn.LeakyReLU(0.2, inplace = True),  
        nn.BatchNorm2d(128), 
        nn.Conv2d(128, 256, 5, 2, 2, bias = False), 
        nn.LeakyReLU(0.2, inplace = True), 
        nn.BatchNorm2d(256), 
        nn.Conv2d(256, 512, 5, 2, 2, bias = False), 
        nn.LeakyReLU(0.2, inplace = True), 
        nn.BatchNorm2d(512) 
    ) 
    self.l = nn.Linear(2048, 1) 
  def forward(self, input): 
    ans = self.d(input) 
    #print(ans.shape) 
    ans1 = torch.flatten(ans, start_dim=1) 
    #print(ans1.shape) 
    ans = self.l(ans1) 
    #print(ans.shape) 
    return ans

#!g1.1
netGen = Generator()
netGen.apply(weight_init)

#!g1.1
netDis = Discriminator()
netDis.apply(weight_init)

#!g1.1
criterion = nn.BCEWithLogitsLoss()
optDis = optim.Adam(netDis.parameters(), lr=0.0002, betas = (0.5, 0.999))
optGen = optim.Adam(netGen.parameters(), lr=0.0002, betas = (0.5, 0.999))

#!g1.1
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

#!g1.1
netDis.to(device)
netGen.to(device)

#!g1.1 
real_score = [] 
fake_score = [] 
epochs = 250 
losses_g = [] 
losses_d = [] 
for epoch in range(epochs): 
  loss_d_tmp = [] 
  loss_g_tmp = [] 
  real_score_tmp = [] 
  fake_score_tmp = [] 
  for real_image, i in tqdm(train_loader): 
    netDis.zero_grad() 
    real_image = real_image.to(device) 
    real_image = Variable(real_image) 
    #print("real_image shape:", real_image.shape) 
    real_preds = netDis(real_image) 
    #присвоим реальному изображению метку класса 1 
    real_targets = Variable(torch.ones(real_image.size()[0], 1, device=device))  
    #учим дисриминатор предсказывать эту метку класса 
    #print(real_preds.shape) 
    #print(real_targets.shape) 
    real_loss = criterion(real_preds, real_targets) 
    #берем латентный вектор 
    with torch.no_grad():     
        latent = Variable(torch.randn(real_image.size()[0], latent_size, 1, 1, device=device)) 
        #генерируем изображение 
        fake_image = netGen(latent) 
    #print(fake_image.shape) 
    #присваимваем метку класса 0 
    fake_targets = Variable(torch.zeros(real_image.size()[0], 1, device=device)) 
    #подвем в дискриминатор, считаем лосс 
    #print("fake_image shape:", fake_image.shape) 
    fake_preds = netDis(fake_image) 
    fake_loss = criterion(fake_preds, fake_targets) 
    #считаем скор и делаем шаг 
    real_score_tmp.append(torch.mean(real_preds).item()) 
    fake_score_tmp.append(torch.mean(fake_preds).item()) 
    loss_d = real_loss + fake_loss 
    loss_d.backward() 
    optDis.step() 
    loss_d_tmp.append(loss_d.item()) 
    #обучим генератор 
    netGen.zero_grad() 
    #генерируем изображение 
    latent = torch.randn(real_image.size()[0], latent_size, 1, 1, device=device) 
    fake_image = netGen(latent) 
         
    #подаем изображение дискриминатору  
    preds = netDis(fake_image) 
    targets = Variable(torch.ones(real_image.size()[0], 1, device=device)) 
    #считаем лосс генератора  
    #print(preds.shape) 
    #print(targets.shape) 
    loss_g = criterion(preds, targets) 
    # делаем шаг 
    loss_g.backward() 
    optGen.step() 
    loss_g_tmp.append(loss_g.item()) 
  if (epoch % 5 == 0):   
    fake_image = netGen(latent)  
    vutils.save_image(fake_image.data, '%s/fake_samples_epoch_%03d.png' % ("data/", epoch), normalize = True) 
    #torch.save(netGen, "model_gen" + str(epoch) + ".zip") 
    #torch.save(netDis, "model_dis" + str(epoch) + ".zip") 
    #torch.save(optDis, "optDis" + str(epoch) + ".zip") 
    #torch.save(optGen, "optGen" + str(epoch) + ".zip") 
  print(f'loss generator on {epoch} = {np.mean(loss_g_tmp)}') 
  print(f'real scores on {epoch} = {np.mean(real_score_tmp)}') 
  print(f'fake scores on {epoch} = {np.mean(fake_score_tmp)}') 
  losses_g.append(np.mean(loss_g_tmp)) 
  losses_d.append(np.mean(loss_d_tmp)) 
  real_score.append(np.mean(real_score_tmp)) 
  fake_score.append(np.mean(fake_score_tmp))

#!g1.1
plt.figure(figsize=(15, 6))
plt.plot(losses_d, '-')
plt.plot(losses_g, '-')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['Discriminator', 'Generator'])
plt.title('Losses');

# Commented out IPython magic to ensure Python compatibility.
#!g1.1
# %pip install pytorch-gan-metrics

#!g1.1
from torchvision.models.inception import inception_v3
import numpy as np
from pytorch_gan_metrics import get_inception_score

#!g1.1
class GeneratorDataset(Dataset):
    def __init__(self, G, z_dim):
        self.G = G
        self.z_dim = z_dim
    
    def __len__(self):
        return 50000
    
    def __getitem__(self, index):
        return self.G(torch.randn(64, latent_size, 1, 1, device=device))[0]

dataset = GeneratorDataset(netGen, latent_size)
loader = DataLoader(dataset, batch_size=64, num_workers=0)

#!g1.1
IS, IS_std = get_inception_score(loader)

#!g1.1
print(IS)