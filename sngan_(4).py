# -*- coding: utf-8 -*-
"""sngan (4).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Klkm5--CqfcIpxT8f3y7l7Lxq0VXPBS9
"""

#!g1.1
from __future__ import print_function
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch.autograd import Variable
import torch
import torchvision
from torchvision import transforms
import matplotlib.pyplot as plt
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
import torch.optim as optim
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
from tqdm.notebook import tqdm
import os
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
import torchvision.transforms as tt
from torchvision.utils import save_image
from torchvision.utils import make_grid
from torchvision.models.inception import inception_v3
import numpy as np
from pytorch_gan_metrics import get_inception_score

#!g1.1
batchsize = 128
latent_size = 100

#!g1.1
training_data = dset.CIFAR10(
    root='./data',
    train=True,
    download=True,
    transform=transforms.Compose ([ 
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        transforms.Lambda(lambda x: x + torch.rand_like(x) / 128)
        ])
)

#!g1.1
train_loader = torch.utils.data.DataLoader(training_data, batch_size = batchsize, shuffle=True, num_workers=4, drop_last=True)

import torch.nn.init as init
from torch.nn.utils.spectral_norm import spectral_norm

#!g1.1
#!g1.1 
class Generator(nn.Module): 
  def __init__(self): 
    super(Generator, self).__init__() 
    self.g = nn.Sequential( 
        nn.ConvTranspose2d(100, 1024, 2, 1, 0, bias = False), 
        nn.BatchNorm2d(1024), 
        nn.ReLU(True), 
        nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias = False), 
        nn.BatchNorm2d(512), 
        nn.ReLU(True), 
        nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False), 
        nn.BatchNorm2d(256), 
        nn.ReLU(True), 
        nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False), 
        nn.BatchNorm2d(128), 
        nn.ReLU(True), 
        nn.ConvTranspose2d(128, 3, 4, 2, 1, bias = False), 
        nn.Tanh() 
    ) 
  self.initialize_weight()
  def initialize_weight(self):
      for p in self.modules():
          if isinstance(p, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):
              init.normal_(p.weight, std=0.02)
              init.zeros_(p.bias)  
  def forward(self, input): 
    return self.g(input.view(-1, 100, 1, 1))

#!g1.1 
class Discriminator(nn.Module): 
  def __init__(self): 
    super(Discriminator, self).__init__() 
    self.d = nn.Sequential( 
        nn.Conv2d(3, 64, 5, 2, 2, bias = False), 
        nn.LeakyReLU(0.2, inplace = True), 
        nn.Conv2d(64, 128, 5, 2, 2, bias = False), 
        nn.LeakyReLU(0.2, inplace = True),  
        nn.BatchNorm2d(128), 
        nn.Conv2d(128, 256, 5, 2, 2, bias = False), 
        nn.LeakyReLU(0.2, inplace = True), 
        nn.BatchNorm2d(256), 
        nn.Conv2d(256, 512, 5, 2, 2, bias = False), 
        nn.LeakyReLU(0.2, inplace = True), 
        nn.BatchNorm2d(512) 
    ) 
    self.l = nn.Linear(2048, 1) 
    self.initialize_weight()
  def initialize_weight(self):
    for p in self.modules():
        if isinstance(p, (nn.Conv2d, nn.Linear)):
            init.normal_(p.weight, std=0.02)
            spectral_norm(p)
  
  def forward(self, input): 
    ans = self.d(input) 
    ans1 = torch.flatten(ans, start_dim=1) 
    ans = self.l(ans1) 
    return ans

#!g1.1
netGen = Generator()

#!g1.1
netDis = Discriminator()

#!g1.1
optDis = optim.Adam(netDis.parameters(), lr=2e-4, betas = [0.0, 0.9])
optGen = optim.Adam(netGen.parameters(), lr=2e-4, betas = [0.0, 0.9])

#!g1.1
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

#!g1.1
netDis.to(device)
netGen.to(device)

real_score = [] 
fake_score = [] 
losses_g = [] 
losses_d = []

#!g1.1
epochs = 100
for epoch in range(epochs):
    for real_image, i in tqdm(train_loader):  
        loss_d_tmp = [] 
        loss_g_tmp = [] 
        real_score_tmp = [] 
        fake_score_tmp = []   
        for i in range(5):
            with torch.no_grad():
                latent = torch.randn(real_image.size()[0], latent_size, device=device)
                latent = latent.to(device) 
                fake_image = netGen(latent).detach()
            real_image = real_image.to(device) 
            real_preds = netDis(real_image).reshape(-1)
            fake_preds = netDis(fake_image).reshape(-1)
            loss_critic = -real_preds.mean() + fake_preds.mean()
            optDis.zero_grad() 
            loss_critic.backward(retain_graph=True)
            optDis.step()
            for p in netDis.parameters():
                p.data.clamp_(-0.01, 0.01)
        latent = torch.randn(real_image.size()[0], latent_size, device=device)
        fake_image = netGen(latent)
        fake_image = netDis(fake_image).reshape(-1) 
        gen_loss = -fake_image.mean()
        optGen.zero_grad() 
        gen_loss.backward()
        optGen.step()
        real_score_tmp.append(torch.mean(real_preds).item()) 
        fake_score_tmp.append(torch.mean(fake_preds).item()) 
        loss_g_tmp.append(gen_loss.item()) 
    if (epoch % 10 == 0):   
        with torch.no_grad():
            latent = torch.randn(real_image.size()[0], latent_size, device=device)
            fake_image = netGen(latent).detach()
            vutils.save_image(fake_image.data, '%s/fake_samples_epoch_%03d.png' % ("data/", epoch+111), normalize = True) 
            if (epoch % 50 == 0 and epoch!=0 and epoch!=100 and epoch != 50):   
                dataset = GeneratorDataset(netGen, latent_size)
                loader = DataLoader(dataset, batch_size=64, num_workers=0)
                IS, IS_std = get_inception_score(loader)
                print(IS)
        torch.save(netGen, "model_gen" + str(epoch+111) + ".zip") 
        torch.save(netDis, "model_dis" + str(epoch+111) + ".zip") 
        torch.save(optDis, "optDis" + str(epoch+111) + ".zip") 
        torch.save(optGen, "optGen" + str(epoch+111) + ".zip") 
        print(f'loss generator on {epoch+111} = {np.mean(loss_g_tmp)}') 
        print(f'real scores on {epoch+111} = {np.mean(real_score_tmp)}') 
        print(f'fake scores on {epoch+111} = {np.mean(fake_score_tmp)}') 
        losses_g.append(np.mean(loss_g_tmp)) 
        losses_d.append(np.mean(loss_d_tmp)) 
        real_score.append(np.mean(real_score_tmp)) 
        fake_score.append(np.mean(fake_score_tmp))

#!g1.1
class GeneratorDataset(Dataset):
    def __init__(self, G, z_dim):
        self.G = G
        self.z_dim = z_dim
    
    def __len__(self):
        return 10000
    
    def __getitem__(self, index):
        return self.G(torch.randn(64, latent_size, 1, 1, device=device))[0]

dataset = GeneratorDataset(netGen, latent_size)
loader = DataLoader(dataset, batch_size=64, num_workers=0)
IS, IS_std = get_inception_score(loader)
print(IS)

latent = torch.randn(real_image.size()[0], latent_size, device=device)
fake_image = netGen(latent).detach()
vutils.save_image(fake_image.data, '%s/fake_samples_epoch_%03d.png' % ("home/aalanov/Kiryanova_Olga/", 250), normalize = True)